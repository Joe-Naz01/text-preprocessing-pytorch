{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW3iBuyNlxSH",
        "outputId": "9ccf6e4e-553e-4b14-a126-28d9376fed91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"numpy<1.25\"\n",
        "!pip install -q torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 torchtext==0.17.2\n",
        "!pip install -q nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"numpy<2.2\"\n"
      ],
      "metadata": {
        "id": "tKEFr-Xxx2hN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eebf0985",
        "outputId": "7716f462-58ac-4139-9339-91f7922da43f"
      },
      "source": [
        "# Import the necessary functions\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from nltk.probability import FreqDist"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1537327943.py\", line 2, in <cell line: 0>\n",
            "    from torchtext.data.utils import get_tokenizer\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchtext/__init__.py\", line 3, in <module>\n",
            "    from torch.hub import _get_torch_home\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZING"
      ],
      "metadata": {
        "id": "DFonMq-vy9Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
        "\n",
        "# Initialize the tokenizer and tokenize the text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "threshold = 1\n",
        "# Remove rare words and print common tokens\n",
        "freq_dist = FreqDist(tokens)\n",
        "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
        "print(common_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBKiPXd-oo-S",
        "outputId": "ff9ea5d5-9f37-40e6-bc29-6c5c619a1604"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'of', ',', 'data', 'alex', 'data', '.', ',', 'alex', ',', 'the', 'data', ',', '.', 'the', 'of', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The moor is very sparsely inhabited, and those who live near each other are thrown very much together. For this reason I saw a good deal of Sir Charles Baskerville. With the exception of Mr. Frankland, of Lafter Hall, and Mr. Stapleton, the naturalist, there are no other men of education within many miles. Sir Charles was a retiring man, but the chance of his illness brought us together, and a community of interests in science kept us so. He had brought back much scientific information from South Africa, and many a charming evening we have spent together discussing the comparative anatomy of the Bushman and the Hottentot\"\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl7NWYVqtw0J",
        "outputId": "9d021659-e169-479b-dad5-7b0494df51f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'moor', 'is', 'very', 'sparsely', 'inhabited', ',', 'and', 'those', 'who', 'live', 'near', 'each', 'other', 'are', 'thrown', 'very', 'much', 'together', '.', 'for', 'this', 'reason', 'i', 'saw', 'a', 'good', 'deal', 'of', 'sir', 'charles', 'baskerville', '.', 'with', 'the', 'exception', 'of', 'mr', '.', 'frankland', ',', 'of', 'lafter', 'hall', ',', 'and', 'mr', '.', 'stapleton', ',', 'the', 'naturalist', ',', 'there', 'are', 'no', 'other', 'men', 'of', 'education', 'within', 'many', 'miles', '.', 'sir', 'charles', 'was', 'a', 'retiring', 'man', ',', 'but', 'the', 'chance', 'of', 'his', 'illness', 'brought', 'us', 'together', ',', 'and', 'a', 'community', 'of', 'interests', 'in', 'science', 'kept', 'us', 'so', '.', 'he', 'had', 'brought', 'back', 'much', 'scientific', 'information', 'from', 'south', 'africa', ',', 'and', 'many', 'a', 'charming', 'evening', 'we', 'have', 'spent', 'together', 'discussing', 'the', 'comparative', 'anatomy', 'of', 'the', 'bushman', 'and', 'the', 'hottentot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STEMMING"
      ],
      "metadata": {
        "id": "DjrBFQZNzAn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Download the stopwords data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize and tokenize the text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "# Remove any stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Perform stemming on the filtered tokens\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwfCCADVt14Z",
        "outputId": "3809b45a-7e22-424a-c19f-f2161334981c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['moor', 'spars', 'inhabit', ',', 'live', 'near', 'thrown', 'much', 'togeth', '.', 'reason', 'saw', 'good', 'deal', 'sir', 'charl', 'baskervil', '.', 'except', 'mr', '.', 'frankland', ',', 'lafter', 'hall', ',', 'mr', '.', 'stapleton', ',', 'naturalist', ',', 'men', 'educ', 'within', 'mani', 'mile', '.', 'sir', 'charl', 'retir', 'man', ',', 'chanc', 'ill', 'brought', 'us', 'togeth', ',', 'commun', 'interest', 'scienc', 'kept', 'us', '.', 'brought', 'back', 'much', 'scientif', 'inform', 'south', 'africa', ',', 'mani', 'charm', 'even', 'spent', 'togeth', 'discuss', 'compar', 'anatomi', 'bushman', 'hottentot']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy\n",
        "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
        "\n",
        "# Define the size of the vocabulary\n",
        "vocab_size = len(genres)\n",
        "\n",
        "# Create one-hot vectors\n",
        "one_hot_vectors = torch.eye(vocab_size)\n",
        "\n",
        "# Create a dictionary mapping genres to their one-hot vectors\n",
        "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
        "\n",
        "for genre, vector in one_hot_dict.items():\n",
        "    print(f'{genre}: {vector.tolist()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blxsvVx1wjsu",
        "outputId": "dd41ac5a-95ce-4486-9a1d-e5a6c6827704"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fiction: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Non-fiction: [0.0, 1.0, 0.0, 0.0, 0.0]\n",
            "Biography: [0.0, 0.0, 1.0, 0.0, 0.0]\n",
            "Children: [0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "Mystery: [0.0, 0.0, 0.0, 0.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BAG OF WORDS"
      ],
      "metadata": {
        "id": "jKQ_P-A-zFZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import from sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
        "\n",
        "# Initialize Bag-of-words with the list of book titles\n",
        "vectorizer = CountVectorizer()\n",
        "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
        "\n",
        "# Extract and print the first five features\n",
        "print(vectorizer.get_feature_names_out()[:5])\n",
        "print(bow_encoded_titles.toarray()[0, :5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHqnr7_FyT9L",
        "outputId": "476ba4fd-b5c7-47df-f060-d5cee844fb99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1984' 'catcher' 'expectations' 'gatsby' 'great']\n",
            "[0 0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing TF-IDF from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "descriptions = ['This is the first doc', 'This is the second doc?', 'This is the third doc','This is the last one!']\n",
        "# Initialize TF-IDF encoding vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_encoded_descriptions = vectorizer.fit_transform(descriptions)\n",
        "\n",
        "# Extract and print the first five features\n",
        "print(vectorizer.get_feature_names_out()[:5])\n",
        "print(tfidf_encoded_descriptions.toarray()[0, :5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEfOvtxByuiA",
        "outputId": "f251090a-c1af-41b7-b428-d29fb91d0929"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doc' 'first' 'is' 'last' 'one']\n",
            "[0.42796959 0.67049706 0.34989318 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING A TEXT PROCESSING PIPELINE"
      ],
      "metadata": {
        "id": "NeJzeNn72Tgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare = ['If it were done when \\'tis done, then \\'twere well It were done quickly. If th\\' assassination Could trammel up the consequence, and catch With his surcease success; that but this blow Might be the be-all and the end-all—here, But here, upon this bank and shoal of time, We\\’d jump the life to come. But in these cases We still have judgement here; that we but teach Bloody instructions, which being taught, return To plague th\\’ inventor. This even-handed justice Commends th\\’ ingredience of our poison\\’d chalice To our own lips. He\\’s here in double trust: First, as I am his kinsman and his subject, Strong both against the deed; then, as his host, Who should against his murderer shut the door, Not bear the knife myself. Besides, this Duncan Hath borne his faculties so meek, hath been So clear in his great office, that his virtues Will plead like angels, trumpet-tongued, against The deep damnation of his taking-off; And pity, like a naked new-born babe, Striding the blast, or heaven\\’s cherubin, hors\\’d Upon the sightless couriers of the air, Shall blow the horrid deed in every eye, That tears shall drown the wind.—I have no spur To prick the sides of my intent, but only Vaulting ambition, which o\\’erleaps itself And falls on th\\’ other—']\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a list of stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Initialize the tokenizer and stemmer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Complete the function to preprocess sentences\n",
        "def preprocess_sentences(sentences):\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "\t\t# Tokenize the sentence\n",
        "        tokens = tokenizer(sentence)\n",
        "\t\t# Remove stop words\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\t\t# Stem the tokens\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "        processed_sentences.append(' '.join(tokens))\n",
        "    return processed_sentences\n",
        "\n",
        "processed_shakespeare = preprocess_sentences(shakespeare)\n",
        "print(processed_shakespeare[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLFtEWT02W44",
        "outputId": "2d47082a-de1e-4281-c478-d00aad6ab6ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"done ' ti done , ' twere well done quickli . th ' assassin could trammel consequ , catch surceas success blow might be-al end-all—her , , upon bank shoal time , we\\\\’d jump life come . case still judgement teach bloodi instruct , taught , return plagu th\\\\’ inventor . even-hand justic commend th\\\\’ ingredi poison\\\\’d chalic lip . he\\\\’ doubl trust first , kinsman subject , strong deed , host , murder shut door , bear knife . besid , duncan hath born faculti meek , hath clear great offic , virtu plead like angel , trumpet-tongu , deep damnat taking-off piti , like nake new-born babe , stride blast , heaven\\\\’ cherubin , hors\\\\’d upon sightless courier air , shall blow horrid deed everi eye , tear shall drown wind . —i spur prick side intent , vault ambit , o\\\\’erleap fall th\\\\’ other—\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your Dataset class\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import torch\n",
        "\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Complete the encoding function\n",
        "def encode_sentences(sentences):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    return X.toarray(), vectorizer\n",
        "\n",
        "# Complete the text processing pipeline\n",
        "def text_processing_pipeline(sentences):\n",
        "    processed_sentences = preprocess_sentences(sentences)\n",
        "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
        "    # Convert the NumPy array to a PyTorch tensor\n",
        "    encoded_tensors = torch.tensor(encoded_sentences, dtype=torch.float32)\n",
        "    dataset = ShakespeareDataset(encoded_tensors)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "    return dataloader,vectorizer\n",
        "\n",
        "dataloader, vectorizer = text_processing_pipeline(shakespeare)\n",
        "\n",
        "# Print the vectorizer's feature names and the first 10 components of the first item\n",
        "print(vectorizer.get_feature_names_out()[:10])\n",
        "print(next(iter(dataloader))[0] [:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1_XdxpT9O6m",
        "outputId": "7491e425-fb6e-4dc2-9077-84ed0f502e8d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['air' 'al' 'all' 'ambit' 'angel' 'assassin' 'babe' 'bank' 'be' 'bear']\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    }
  ]
}